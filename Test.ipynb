{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Pre-Processing Text data"
      ],
      "metadata": {
        "id": "QrisBrsM154d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjAipvce1hUf"
      },
      "outputs": [],
      "source": [
        "# Load the test dataset\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/test.csv\")\n",
        "test_df = test_df[['crimeaditionalinfo', 'category', 'sub_category']].dropna()\n",
        "\n",
        "# Combine category and sub_category into a single target for evaluation\n",
        "test_df['target'] = test_df.apply(lambda row: f\"category: {row['category']} sub_category: {row['sub_category']}\", axis=1)\n",
        "\n",
        "# Tokenize the test dataset\n",
        "def preprocess_test_data(examples):\n",
        "    inputs = tokenizer(examples['crimeaditionalinfo'], max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    return inputs\n",
        "\n",
        "test_inputs = preprocess_test_data({\"crimeaditionalinfo\": test_df['crimeaditionalinfo'].tolist()})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prediction and Accuracy"
      ],
      "metadata": {
        "id": "eNIoZ6M91-X5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Function to calculate metrics in batches\n",
        "def calculate_metrics_in_batches(model, tokenizer, test_df, batch_size=8, device='cuda'):\n",
        "    model.to(device)\n",
        "    model.eval()  # Ensure the model is in evaluation mode\n",
        "\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "\n",
        "    for i in range(0, len(test_df), batch_size):\n",
        "        batch = test_df.iloc[i:i+batch_size]\n",
        "        inputs = tokenizer(\n",
        "            batch['crimeaditionalinfo'].tolist(),\n",
        "            max_length=128,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_ids=inputs['input_ids'],\n",
        "                attention_mask=inputs['attention_mask'],\n",
        "                max_length=64,\n",
        "                num_beams=4,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "        # Decode predictions and collect ground truth\n",
        "        predictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        targets = batch['target'].tolist()\n",
        "\n",
        "        # Append predictions and ground truth\n",
        "        pred_labels.extend([pred.strip() for pred in predictions])\n",
        "        true_labels.extend([target.strip() for target in targets])\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(true_labels, pred_labels)\n",
        "    precision = precision_score(true_labels, pred_labels, average='weighted')\n",
        "    recall = recall_score(true_labels, pred_labels, average='weighted')\n",
        "    f1 = f1_score(true_labels, pred_labels, average='weighted')\n",
        "\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Usage\n",
        "accuracy, precision, recall, f1 = calculate_metrics_in_batches(model, tokenizer, test_df, batch_size=8, device='cuda')\n",
        "print(f\"Test Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "id": "M0edVYoR1nIc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}